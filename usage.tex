\section{Usage Scenarios}
\label{sec:scenarios}

Quantitative consistency metrics are useful for a variety of services
that can tolerate eventual consistency. In this section, we outline
three use cases in the context of a hypothetical microblogging web
service, Twissistency.

\subsection{Dynamic Reconfiguration}
\label{sec:dynamic}

Without quantitative guidance, choosing the correct values for
replication parameters is a difficult task. We believe that, instead
of reasoning about low-level replication settings, application writes
should instead declaratively specify their objectives in the form of
higher-level service agreements and allow the system to adjust its
parameters to meet this goal.

Data stores often offer a choice between replication parameters. For
example, in Dynamo-style sytems like Cassandra, applications can
choose the minimum number of replicas to read from ($R$) and write to
($W$). If $R$+$W$ is greater than the number of replicas ($N$),
operations will achieve \textit{strong consistency}, and, if not, the
system provides \textit{eventual consistency}. With a replication
factor of $N=3$, we have three options for eventually consistent
operation: $R$$=$$W$$=$$1$, $R$$=$$1$$, W$$=$$2$, and $R$$=$$2$$,
W$$=$$1$.  $R$$=$$W$$=$$1$ is guaranteed to be fastest, but it is also
the least consistent. Instead of thinking about $R$ and $W$, the
developers can think in terms of desired $t$-visibility:

\vignette{Twissistency's data scientists have learned that their users
  respond negatively to slow update propagation, and the company sets
  a $t$-visibility target of 500ms at the 99.9th percentile for their
  back-end data store requests (while minimizing latency).}

How should we configure the replication parameters for a given SLA?
One approach is to manually tune the system---this is straightforward
but is not necessarily robust to changing conditions:

\vignette{Based on feedback from their infrastructure team, the
  developers set $R$=$W$=$1$ for their Dynamo-based data store. This
  meets the $t$-visibility consistency SLA, but, during peak traffic,
  the consistency SLA is sporadically violated.}

Instead, we can auto-tune the replication parameters for each
request. Consistency monitoring allows us to monitor SLA violations,
while consistency prediction allows us the system to do forward testing of
replication parameter changes before making them:

\vignette{The consistency autotuner uses R=W=1 for most traffic but
  switches to R=2, W=1 during peak workload times. While R=1, W=2 is
  also a viable solution, the autotuner determines that the 99.9th
  percentile operation latency would suffer since most reads are
  served from the data store's buffer cache.}

While the literature suggests several dynamic replication
schemes~\cite{vahdat-article}, we are pleased that these
techniques are now a reality for production-ready data stores.

\subsection{DBA Integration}
\label{sec:dba}

Consistency metrics can also be used in diagnostic tasks and to
understand \textit{why} a system is misbehaving. There are a number of
system parameters which affect the performance and observed
consistency of a distributed data store. System administrators
currently face two major feature shortcomings: there is often limited
information available in terms of the currently observed consistency
properties and there is a lack of easy methods to understand how the
system will behave as parameters are varied.

\vignette{The database administrators at Twissistency have received
  reports that a high-profile user is seeing very stale data.}

There are a host of consistency configuration options available to
data store administrators. Taking Cassandra as an example, the administrators can
configure read repair rates, perform active anti-entropy value
exchange, and enable or disable replicas. Moreover, there are many
causes for inconsistent reads: there may be slow nodes in the cluster
or that there are some keys which are hotspots.

\vignette{The administrators inspect the consistency metrics for each
  data store shard and identify a misbehaving set of nodes
  corresponding to a bad top-of-rack switch. They temporarily increase
  the rate of background version exchange for the shard and begin to
  spin up a new replica set on a different rack before rebooting the
  switch.}

We believe consistency metrics should allow standard analytics such as
fine-grained drill-downs and roll-ups across both logical data items
and physical-layer details like placement and hardware details. If, as
in our Cassandra implementation (Section~\ref{sec:architecture}),
monitoring is performed as a white-box, in-database service, low-level
details like network topologies and per-operation latencies will be
available to the end-user. Of course, it may be sufficient for most
users to simply experiment with common configuration parameters via
prediction, but exposing this advanced analytics functionality is
likely useful for power users.

\subsection{Monitoring and Alerts}
\label{sec:monitoring}

Consistency metrics allow new approaches to traditional monitoring:

\vignette{Twissistency has a large number of DevOps who are
  responsible for keeping their service up and running. Currently
  their monitoring and pager service is triggered when operation
  latency is high or if servers fail. As user experience is negatively
  impacted by inconsistent reads, the CTO wants to ensure that DevOps
  respond to these alerts.}

As we discussed in Section~\label{sec:dynamic}, some parameters like
per-request quorum settings, are amenable to SLA-based automatic
control. However, there are a number of scenarios where traditional,
manual monitor-and-respond is an acceptable approach. If SLAs cannot
be met under any circumstances (e.g., operation latency and
$t$-visibility bounds are too restrictive), the correct response is to
revise the SLAs or perform more invasive operations (e.g., add more
replicas) which may require active human oversight.
